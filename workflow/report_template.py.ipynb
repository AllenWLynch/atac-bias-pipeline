{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style = 'ticks')\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "from io import StringIO\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from anndata import AnnData\n",
    "import anndata\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "import subprocess\n",
    "from functools import partial\n",
    "\n",
    "#__DATA READING__\n",
    "def get_peak_id(data, keys = ['chr','start','end']):\n",
    "    return data[keys[0]].astype(str) + '_' + data[keys[1]].astype(str).str.strip() + '_' + data[keys[2]].astype(str).str.strip()\n",
    "\n",
    "def read_bias_file(filepath):\n",
    "    colnames = 'chr,start,end,barcode,dup_group,bias1,bias2,gc_content,fragment_len,log_duprate,peak_chr,peak_start,peak_end,corrected_count'.split(',')\n",
    "    fragments = pd.read_csv(filepath, header = None, sep = '\\t')\n",
    "    fragments.columns = colnames\n",
    "    fragments['peak_id'] = get_peak_id(fragments, keys = ['peak_chr','peak_start','peak_end'])\n",
    "    return fragments\n",
    "\n",
    "def read_stats(filepath, columns):\n",
    "    stats = pd.read_csv(filepath, sep = '\\t', header = None)\n",
    "    stats.columns = columns\n",
    "    #stats['samples'] = stats.samples.apply(lambda x : list(map(float, x.strip('[]').split(', ')))).apply(np.array)\n",
    "    return stats\n",
    "\n",
    "def read_barcode_stats(filepath):\n",
    "    return read_stats(filepath, ['barcode','mean_log_duprate','fragment_count','samples'])\n",
    "\n",
    "def read_peak_stats(filepath):\n",
    "    peak_stats = read_stats(filepath, ['peak_id','median_log_duprate','fragment_count','samples'])\n",
    "    peak_stats['rank'] = peak_stats.median_log_duprate.rank(method = 'first',na_option = 'keep')\n",
    "    return peak_stats\n",
    "\n",
    "def read_sparse_countmatrix(barcode_file, peaks_file, count_matrix_file, min_peak_proportion = 0):\n",
    "\n",
    "    barcode_stats = read_barcode_stats(barcode_file)\n",
    "    peak_stats = read_peak_stats(peaks_file)\n",
    "    \n",
    "    counts = sparse.load_npz(count_matrix_file)\n",
    "\n",
    "    data = AnnData(X = counts, obs = barcode_stats, var = peak_stats)\n",
    "\n",
    "    data.var['accessible_in_cells'] =  np.array((data.X > 0).sum(axis=0)).reshape(-1)\n",
    "\n",
    "    return data\n",
    "\n",
    "#__PLOT GENERATION__\n",
    "def benchmark_fragment_model(fragments, dupcounts, peak_stats):\n",
    "\n",
    "    fragments = fragments.merge(peak_stats[['peak_id','accessible_in_cells']], on = 'peak_id', how = 'inner')\\\n",
    "        .merge(dupcounts, left_on = 'dup_group', right_on = 'group')\n",
    "    fragments['true_log_duprate'] = np.log(fragments.duplicate_counts / fragments.accessible_in_cells)\n",
    "\n",
    "    return fragments[['log_duprate', 'true_log_duprate']]\n",
    "\n",
    "def sample_ranks(andata_chunk, num_samples = 10000):\n",
    "\n",
    "    cluster_peak_counts = np.array(andata_chunk.X.sum(axis = 0)).reshape(-1)\n",
    "    cluster_peak_probs = cluster_peak_counts / cluster_peak_counts.sum()\n",
    "\n",
    "    peak_samples = np.random.choice(len(cluster_peak_probs), p = cluster_peak_probs, \n",
    "        size = (min(len(cluster_peak_probs), num_samples),))\n",
    "    \n",
    "    sampled_ranks = andata_chunk.var.iloc[peak_samples]['rank'].values.astype(int)\n",
    "\n",
    "    return sampled_ranks\n",
    "\n",
    "def get_bias_peak_enrichment(andata, cluster_key, num_samples = 10000):\n",
    "\n",
    "    ranks_dict = {}\n",
    "    for cluster_id in np.unique(andata.obs[cluster_key]):\n",
    "        ranks_dict[cluster_id] = sample_ranks(\n",
    "            andata[andata.obs[cluster_key] == cluster_id, ~np.isnan(andata.var['rank'])], \n",
    "            num_samples=num_samples\n",
    "        )\n",
    "\n",
    "    ranks_df = pd.DataFrame(ranks_dict)\n",
    "    ranks_df = pd.melt(ranks_df, var_name = 'cluster_id', value_name = 'rank')\n",
    "    return ranks_df\n",
    "\n",
    "def get_fragment_distribution_by_peak_and_cluster(andata, peaks_str, cluster_key):\n",
    "\n",
    "    selected_fragments = read_bias_file(StringIO(peaks_str))\n",
    "\n",
    "    selected_fragments = selected_fragments.merge(andata.obs[['barcode',cluster_key]], on = 'barcode', how = 'right')\\\n",
    "        .merge(andata.var[['peak_id','rank']], on = 'peak_id', how = 'right')\n",
    "\n",
    "    stratified_subset = selected_fragments.groupby(['peak_id',cluster_key], group_keys=False)\\\n",
    "        .apply(lambda x : x.sample(min(len(x), 150)))\n",
    "\n",
    "    return stratified_subset\n",
    "\n",
    "#__SCANPY ADDONS___\n",
    "def get_differential_peaks(andata, top_n = 200):\n",
    "\n",
    "    sc.tl.rank_genes_groups(andata, 'leiden', method='wilcoxon')\n",
    "\n",
    "    return pd.DataFrame(andata.uns['rank_genes_groups']['names']).head(200)\n",
    "\n",
    "def standardize(x):\n",
    "    return np.clip((x - x.mean()) / x.std(), -3, 3)\n",
    "\n",
    "def plot_umap(andata,*,color_key, key = 'X_umap', quantitative=True, ax = None, center=True, legend = False,\n",
    "             hue_order = None, hue_norm = None):\n",
    "    data = andata.obsm['X_umap']\n",
    "    if type(color_key) == str:\n",
    "        color_var = andata.obs[color_key].values\n",
    "    else:\n",
    "        color_var = color_key\n",
    "        \n",
    "    if not quantitative:\n",
    "        ax = sns.scatterplot(x = data[:,0], y = data[:,1], size = 0.5, \n",
    "                       hue = color_var.astype('str'), \n",
    "                       hue_order = hue_order, legend = legend, ax = ax)\n",
    "    else:\n",
    "        ax = sns.scatterplot(x = data[:,0], y = data[:,1], hue = standardize(color_var) if center else color_var,\n",
    "                   palette = \"vlag\", size = 0.5, legend = False, ax = ax, hue_norm = hue_norm)\n",
    "    ax.set(xticklabels = [], xticks = [], yticks = [])\n",
    "    sns.despine()\n",
    "    return ax\n",
    "\n",
    "def _binary_search(func, acceptible_range, low, high, iter_num = 0, max_iter = 10):\n",
    "\n",
    "    midpoint = low + (high - low)/2\n",
    "\n",
    "    output = func(midpoint)\n",
    "    \n",
    "    if (output >= acceptible_range[0] and output < acceptible_range[1]) or iter_num >= max_iter:\n",
    "        return midpoint\n",
    "\n",
    "    if output < acceptible_range[0]:\n",
    "        return _binary_search(func, acceptible_range, midpoint, high, iter_num + 1, max_iter=max_iter)\n",
    "    else:\n",
    "        return _binary_search(func, acceptible_range, low, midpoint, iter_num+1, max_iter=max_iter)        \n",
    "        \n",
    "\n",
    "def get_num_clusters(andata, resolution):\n",
    "    sc.tl.leiden(andata, resolution = 10**resolution)\n",
    "    num_clusters = andata.obs.leiden.nunique()\n",
    "    print('Resolution: {}, # Clusters: {}'.format(str(10**resolution), str(num_clusters)))\n",
    "    return num_clusters\n",
    "    \n",
    "def process_counts(andata, resolution = 0.5, tune = False, num_clusters = (7,10)):\n",
    "    \n",
    "    lsi_model = TruncatedSVD(n_components=50)\n",
    "    X_LSI = lsi_model.fit_transform(\n",
    "        TfidfTransformer().fit_transform(andata.X)\n",
    "    )\n",
    "\n",
    "    andata.obsm['X_lsi'] = X_LSI\n",
    "    \n",
    "    sc.pp.neighbors(andata, use_rep = 'X_lsi')\n",
    "\n",
    "    if tune:\n",
    "        cluster_func = partial(get_num_clusters, andata)\n",
    "        resolution = _binary_search(cluster_func, num_clusters, -2, 0)\n",
    "    else:\n",
    "        sc.tl.leiden(andata, resolution = resolution)\n",
    "\n",
    "    sc.tl.umap(andata)\n",
    "    \n",
    "    return lsi_model, resolution\n",
    "\n",
    "def compute_neighborhood_change(*,control, treatment, cluster_key='leiden', mask_cluster = True):\n",
    "    \n",
    "    a1_distances = cosine_similarity(control.obsm['X_lsi'])\n",
    "    a2_distances = cosine_similarity(treatment.obsm['X_lsi'])\n",
    "    \n",
    "    if mask_cluster:\n",
    "        clusters = control.obs[cluster_key].values\n",
    "        cluster_square = np.tile(clusters, (len(clusters), 1))\n",
    "\n",
    "        same_cluster_mask = cluster_square == cluster_square.T\n",
    "    else:\n",
    "        same_cluster_mask = np.full(a1_distances.shape, True)\n",
    "    \n",
    "    a1_distances = normalize(np.where(same_cluster_mask, 0.0, a1_distances), 'l2')\n",
    "    a2_distances = normalize(np.where(same_cluster_mask, 0.0, a2_distances), 'l2')\n",
    "    \n",
    "    neighborhood_change = 1 - np.sum(np.multiply(a1_distances, a2_distances), axis = 1)\n",
    "    \n",
    "    control.obs['neighbor_change'] = neighborhood_change\n",
    "\n",
    "\n",
    "def make_proportion_plot(*,data,x,y,hue, hue_order = None, order = None, figsize=(5,5)):\n",
    "    \n",
    "    if y is None:\n",
    "        y = '__count'\n",
    "        data['__count'] = 1\n",
    "        \n",
    "    factor_counts = data.groupby(x)[y].sum()\n",
    "    \n",
    "    if hue_order is None:\n",
    "        hue_order = sorted(np.unique(data[hue]))\n",
    "        \n",
    "    if order is None:\n",
    "        order = sorted(np.unique(data[x]))\n",
    "        \n",
    "    cumm_prop = {x : 0 for x in order}\n",
    "    \n",
    "    groups = data.groupby([x,hue])\n",
    "    \n",
    "    new_df = []\n",
    "    for y_level in hue_order:\n",
    "        for x_level in order:\n",
    "            level_count = groups.get_group((x_level, y_level))[y].sum()\n",
    "            level_prop = cumm_prop[x_level] + level_count/factor_counts[x_level]\n",
    "            cumm_prop[x_level] = level_prop\n",
    "            new_df.append((x_level, y_level, level_prop))\n",
    "            \n",
    "    new_df = pd.DataFrame(new_df, columns = [x,hue,'proportion'])\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    sns.barplot(data = new_df, x=x, y='proportion',hue =hue, \n",
    "                               hue_order= reversed(hue_order), dodge=False, orient='v',\n",
    "                               palette=sns.color_palette('hls'), linewidth=0.0, ax= ax)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(1.4, 0.8), shadow=False, ncol=1)\n",
    "    sns.despine()\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {}\n",
    "try:\n",
    "    snakemake\n",
    "    output_paths,paths = {},{}\n",
    "    dataset_strlen = len(snakemake.wildcards.dataset)\n",
    "    for name, path in snakemake.input.items():\n",
    "        output_paths[name] = '.' + path[dataset_strlen:]\n",
    "        paths[name] = path\n",
    "        \n",
    "    print('When opening this notebook from dataset directory, path to pickle:')\n",
    "    print('.' + snakemake.output.paths[dataset_strlen:])\n",
    "    with open(snakemake.output.paths, 'wb') as f:\n",
    "        pickle.dump(output_paths, f)\n",
    "except NameError:\n",
    "    with open('./original_analysis/paths.pkl','rb') as f:\n",
    "        paths = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = anndata.read_h5ad(paths['counts'])\n",
    "corrected_data = anndata.read_h5ad(paths['corrected_counts'])\n",
    "dedup_data = anndata.read_h5ad(paths['dedup_counts'])\n",
    "\n",
    "dupcounts = pd.read_csv(paths['dupcounts'], sep = '\\t')\n",
    "sample_fragments = read_bias_file(paths['sample_fragments'])\n",
    "\n",
    "raw_data_lsi_model, resolution = process_counts(raw_data, tune = True)\n",
    "corrected_data_lsi_model, _ = process_counts(corrected_data, 10**resolution)\n",
    "dedup_lsi_model, _ = process_counts(dedup_data, 10**resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fragment Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = benchmark_fragment_model(sample_fragments, dupcounts, raw_data.var)\n",
    "\n",
    "ax = sns.displot(points, x = 'log_duprate', y = 'true_log_duprate', kind = 'kde')\n",
    "ax.set(xlabel = 'Predicted Fragment Count', ylabel = 'Observed Fragment Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare UMAPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue_order = sorted(np.unique(raw_data.obs.leiden))\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize = (14,5))\n",
    "plot_umap(corrected_data, color_key=raw_data.obs.leiden, quantitative=False, ax = ax[0], hue_order=hue_order)\n",
    "ax[0].set(title = 'Bias-corrected UMAP', xlabel = 'Colored by UNcorrected clustering')\n",
    "\n",
    "plot_umap(dedup_data, color_key=raw_data.obs.leiden, quantitative=False, ax = ax[1], legend = False, hue_order=hue_order)\n",
    "ax[1].set(title = 'Deduplicated UMAP',xlabel = 'Colored by UNcorrected clustering')\n",
    "\n",
    "plot_umap(raw_data, color_key=raw_data.obs.leiden, quantitative=False, ax = ax[2], legend = True,\n",
    "         hue_order=hue_order)\n",
    "ax[2].set(title = 'Raw Count UMAP')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe Bias Concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(10,6))\n",
    "ax = sns.scatterplot(data = raw_data.obs, x = 'fragment_count', y ='mean_log_duprate', hue = 'leiden', legend = False,\n",
    "                    hue_order= hue_order, linewidth = 0, s = 10, alpha = 0.8, ax = ax)\n",
    "ax.set(xscale = 'log', xlabel = 'log(Fragment Count)', ylabel = 'Mean log(duplication rate) per cell')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(14,5))\n",
    "\n",
    "plot_umap(raw_data, color_key = 'mean_log_duprate', ax = ax[0])\n",
    "ax[0].set(title = 'Corrected UMAP, Mean Bias Per Cell')\n",
    "plot_umap(raw_data, color_key='fragment_count', ax = ax[1])\n",
    "ax[1].set(title = 'Normal UMAP, Mean Bias Per Cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak-Bias Heterogeneity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, l in enumerate(raw_data.var.dropna().sample(20).samples.values):\n",
    "    for x in l.strip('[]').split(', '):\n",
    "        results.append((i, float(x)))\n",
    "\n",
    "peak_samples = pd.DataFrame(results, columns = ['peak_id','log_duprate'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15,5))\n",
    "sns.violinplot(data = peak_samples, x = 'peak_id', y ='log_duprate', ax = ax)\n",
    "ax.set(xticks = [], ylabel = 'Log(duprate)', xlabel = 'Peaks')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrichment of Biased Peaks Per Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_enrichments = get_bias_peak_enrichment(raw_data, 'leiden')\n",
    "\n",
    "ax = sns.displot(data = cluster_enrichments, hue = 'cluster_id', x = 'rank', \n",
    "            kind = 'kde', common_norm = False, hue_order= hue_order, height = 7, aspect = 1.5)\n",
    "ax.set(xlabel = 'Peak Bias Rank', ylabel = 'Proportion of Fragments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell-Specific Effects Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unbiased_rank, biased_rank = np.quantile(raw_data.var['rank'].dropna().values, [0.3,0.7])\n",
    "peaks_of_interest = pd.concat([\n",
    "    raw_data.var[raw_data.var['rank'] >= biased_rank].sample(10),\n",
    "    raw_data.var[raw_data.var['rank'] <= unbiased_rank].sample(10)\n",
    "])['peak_id'].values\n",
    "\n",
    "peaks_str = '\\n'.join(\n",
    "    peak.replace('_','\\t') for peak in peaks_of_interest\n",
    ")\n",
    "\n",
    "fragment_file = paths['fragments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "relavant_peaks = !bedtools intersect -a $fragment_file -b stdin -sorted < <(echo \"$peaks_str\" | sort -k1,1 -k2,2n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_sample = get_fragment_distribution_by_peak_and_cluster(raw_data, '\\n'.join(relavant_peaks), 'leiden')\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15,5))\n",
    "sns.swarmplot(data = stratified_sample.sort_values('rank'), x = 'peak_id', y = 'log_duprate', hue = 'leiden', \n",
    "              ax = ax, dodge = True, size = 1.5, \n",
    "              order = stratified_sample.groupby('peak_id')['rank'].first().sort_values().index.values)\n",
    "sns.despine()\n",
    "ax.set(xticks = [], ylabel = 'Log(Duplication Rate)', xlabel = 'less bias ← Peaks → more bias')\n",
    "ax.get_legend().remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differential Peaks Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffpeaks = get_differential_peaks(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster, data in diffpeaks.T.iterrows():\n",
    "    with open(os.path.join(snakemake.output.diffpeaks, 'cluster_{}_diffpeaks.bed'.format(str(cluster))), 'w') as f:\n",
    "        print('\\n'.join([x.replace('_','\\t') for x in data.values]), file = f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta Diffpeak Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_data.obs['leiden_corrected'] = corrected_data.obs.leiden\n",
    "sc.tl.rank_genes_groups(corrected_data,'leiden',method='wilcoxon')\n",
    "sc.tl.rank_genes_groups(dedup_data,'leiden_corrected',method='wilcoxon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_peaks = {}\n",
    "print('Cluster','Corrected','Deduplicated', sep = '\\t')\n",
    "for cluster in np.unique(corrected_data.obs.leiden):\n",
    "    diff_corrected = sc.get.rank_genes_groups_df(corrected_data,group=cluster,pval_cutoff=0.05/len(raw_data.obs),\n",
    "                                                 log2fc_min=1.5).sort_values('pvals')['names'].values\n",
    "    diff_dedup = sc.get.rank_genes_groups_df(dedup_data, group=cluster,pval_cutoff=0.05/len(raw_data.obs), \n",
    "                                             log2fc_min=1.5).sort_values('pvals')['names'].values\n",
    "    print('{}'.format(cluster),len(diff_corrected),len(diff_dedup), sep = '\\t')\n",
    "    delta_peaks[cluster] = list(set(diff_corrected).difference(set(diff_dedup)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "density = get_bias_peak_enrichment(corrected_data, 'leiden')\n",
    "\n",
    "ax = sns.displot(data = density, hue = 'cluster_id', x = 'rank', \n",
    "            kind = 'kde', common_norm = False, hue_order= hue_order, height = 7, aspect = 1.5)\n",
    "\n",
    "delta_ranks = pd.DataFrame([(cluster,peak_rank) for cluster, peak_ranks in delta_peaks.items() for peak_rank in peak_ranks], columns = ['cluster','rank'])\n",
    "sns.rugplot(data = delta_ranks, x = 'rank', hue = 'cluster', legend = False, hue_order=hue_order)\n",
    "\n",
    "ax.set(xlabel = 'Peak Bias Rank', ylabel = 'Proportion of Fragments', xticks = [])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_ranks['rank'] = delta_ranks['rank'].astype(np.float32)\n",
    "delta_ranks = delta_ranks.merge(raw_data.var.drop(columns = ['samples']), on = 'rank')\n",
    "\n",
    "delta_ranks[['chr','start','end']] = delta_ranks['peak_id'].str.split('_',expand=True)\n",
    "\n",
    "delta_ranks['strand'] = '*'\n",
    "\n",
    "delta_ranks[['chr','start','end','strand','rank','median_log_duprate','fragment_count','accessible_in_cells']]\\\n",
    "    .to_csv(snakemake.output.delta_diffpeaks, index=None, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak Annotation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(paths['peak_annotations'], sep = '\\t')\n",
    "annotations.annotation = np.where(annotations.annotation.str.contains('Intron'), 'Intron', annotations.annotation)\n",
    "annotations.annotation = np.where(annotations.annotation.str.contains('exon'), 'Exon', annotations.annotation)\n",
    "annotations['start'] = annotations.start - 1\n",
    "annotations['peak_id'] = get_peak_id(annotations, keys = ['seqnames','start','end'])\n",
    "\n",
    "annotations = annotations.merge(raw_data.var[['peak_id','median_log_duprate']], on = 'peak_id')\n",
    "annotations = annotations.rename(columns = {'V4' : \"TAD\"}).drop(columns=['V5','V6'])\n",
    "annotations['bias_bin'], bins = pd.qcut(annotations.median_log_duprate, 3, labels = ['Least Bias','Average','Most Bias'], retbins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_order = ['Promoter (<=1kb)', 'Promoter (1-2kb)', 'Promoter (2-3kb)',\n",
    "                 \"5' UTR\",'Intron','Exon',\"3' UTR\", 'Downstream (<1kb)', 'Downstream (1-2kb)', 'Downstream (2-3kb)',\n",
    "                'Distal Intergenic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Enrichment of genomic features by bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_proportion_plot(data = annotations, hue='annotation',x='bias_bin',y=None,hue_order=feature_order,\n",
    "                         order = ['Least Bias','Average','Most Bias'], figsize=(5,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Distribution of bias by genomic feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,5))\n",
    "sns.violinplot(data = annotations, x = 'annotation', y ='median_log_duprate', ax= ax, order=feature_order, palette=reversed(sns.color_palette('hls')))\n",
    "sns.despine()\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "ax.set(xlabel = 'Peak Annotation',ylabel= 'Median Log Duprate of Peaks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.var = raw_data.var.merge(annotations[['peak_id','annotation', 'TAD']], on = 'peak_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_annotation_matrix = pd.DataFrame(raw_data.X.todense(),\n",
    "        index = raw_data.obs.leiden.values, columns = raw_data.var.annotation.values)\n",
    "\n",
    "cell_annotation_matrix = cell_annotation_matrix.reset_index().melt(id_vars = 'index', value_name='count',var_name='annotation')\n",
    "cell_annotation_matrix = cell_annotation_matrix[cell_annotation_matrix['count'] > 0]\n",
    "\n",
    "cell_groups = cell_annotation_matrix.groupby(['index','annotation']).sum().reset_index()\\\n",
    "    .rename(columns = {'index' : 'cluster ID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_groups_matrix = cell_groups.pivot(index = 'cluster ID', columns = 'annotation', values='count')[feature_order]\n",
    "cell_groups_matrix = cell_groups_matrix / cell_groups_matrix.sum(axis = 0)\n",
    "\n",
    "normed_matrix = (cell_groups_matrix.T - cell_groups_matrix.T.mean(axis = 0)) / cell_groups_matrix.T.std(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig,ax = plt.subplots(1,1,figsize=(6,6))\n",
    "sns.clustermap(normed_matrix, cmap='vlag', square = True)#, ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap(raw_data, color_key='leiden',quantitative=False, legend=True, hue_order=hue_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promoter vs. Enhancer vs. TAD Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.obs.index = raw_data.obs.index.astype(str)\n",
    "raw_data.var.index = raw_data.var.index.astype(str)\n",
    "enhancer_peaks = ~raw_data.var.annotation.str.contains('Promoter')\n",
    "enhancers = raw_data[:, enhancer_peaks]\n",
    "promoters = raw_data[:, ~enhancer_peaks]\n",
    "\n",
    "process_counts(enhancers, 10**resolution)\n",
    "process_counts(promoters, 10**resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tad_domains = raw_data.var.TAD.unique()\n",
    "raw_data.var.index = raw_data.var.index.astype(str)\n",
    "tad_matrix = AnnData( X = np.hstack([\n",
    "    raw_data[:, raw_data.var.TAD == tad].X.sum(axis = 1)\n",
    "    for tad in tad_domains\n",
    "]), obs = pd.DataFrame(raw_data.obs.barcode.values, columns = ['barcode']), var = pd.DataFrame(tad_domains, columns = ['TAD']))     \n",
    "tad_matrix.obs['leiden_raw'] = raw_data.obs.leiden\n",
    "tad_matrix.var = tad_matrix.var.set_index('TAD')\n",
    "\n",
    "tad_matrix.raw = tad_matrix\n",
    "sc.pp.normalize_total(tad_matrix, target_sum=1000)\n",
    "sc.pp.log1p(tad_matrix)\n",
    "sc.pp.scale(tad_matrix)\n",
    "sc.tl.pca(tad_matrix)\n",
    "sc.pp.neighbors(tad_matrix)\n",
    "sc.tl.umap(tad_matrix)\n",
    "sc.tl.leiden(tad_matrix, resolution=10**resolution)\n",
    "sc.tl.rank_genes_groups(tad_matrix, 'leiden_raw', method='wilcoxon', use_raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Different Genomic Resolutions for Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(14,4))\n",
    "plot_umap(enhancers, color_key = raw_data.obs.leiden, quantitative=False, ax = ax[0], hue_order=hue_order)\n",
    "plot_umap(promoters, color_key = raw_data.obs.leiden, quantitative=False, ax = ax[1], hue_order=hue_order)\n",
    "plot_umap(tad_matrix, color_key= raw_data.obs.leiden, quantitative=False, ax = ax[2], hue_order=hue_order)\n",
    "for i, title in zip(range(3), ['Enhancers','Promoters','TADs']):\n",
    "    ax[i].set(title= title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Which cells experience the greatest change in neighborhood, Enhancers vs Promoters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_neighborhood_change(control=enhancers, treatment=promoters, cluster_key='leiden')\n",
    "\n",
    "ax = plot_umap(enhancers, color_key='neighbor_change')\n",
    "ax.set(title = 'Change in neighbors due to treatment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Which types of TADs are discriminative between cell types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_tads = sc.get.rank_genes_groups_df(tad_matrix, '0')\n",
    "tad_activity = raw_data.var.groupby('TAD')['accessible_in_cells'].mean()\n",
    "diff_tads = diff_tads.merge(tad_activity, left_on='names', right_on='TAD')\n",
    "diff_tads['log_10_p'] = - np.log10(diff_tads.pvals)\n",
    "\n",
    "sns.scatterplot(data = diff_tads, x = 'logfoldchanges', y = 'log_10_p', hue = 'accessible_in_cells')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doublets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "doublet_data = pd.read_csv(paths['doublets'], sep = '\\t', header = None)\n",
    "raw_data.obs[['doublet_score','is_doublet']] = doublet_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(12,4))\n",
    "data = raw_data.obsm['X_umap']\n",
    "sns.scatterplot(x = data[:,0], y = data[:,1], hue = standardize(raw_data.obs.doublet_score),\n",
    "           palette = \"vlag\", size = ~raw_data.obs.is_doublet, legend = False, ax = ax[0])\n",
    "ax[0].set(xticklabels = [], xticks = [], yticks = [])\n",
    "sns.despine()\n",
    "plot_umap(raw_data, color_key = raw_data.obs.leiden, quantitative=False, ax = ax[1], hue_order=hue_order)\n",
    "ax[1].set(title = 'Enhancer Clustering, Control Clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(12,4))\n",
    "plot_umap(enhancers, color_key=raw_data.obs.doublet_score, ax = ax[0])\n",
    "ax[0].set(title = 'UMAP with Enhancers Only, colored by doublet score')\n",
    "plot_umap(promoters, color_key=raw_data.obs.doublet_score, ax = ax[1])\n",
    "ax[1].set(title = 'UMAP with Promoters Only, colored by doublet score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhancers.obs['fragment_count'] = enhancers.X.sum(axis = 1)\n",
    "sns.scatterplot(x= enhancers.obs.fragment_count, y = raw_data.obs.doublet_score, hue = raw_data.obs.is_doublet, \n",
    "               palette='vlag')\n",
    "sns.despine()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
